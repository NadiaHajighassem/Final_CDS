{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Assignment 3 - Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import praw\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "import json\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weird letters are the cresidentials from my API\n",
    "\n",
    "reddit = praw.Reddit(client_id ='hQafbgnPnl6Jl_wabPofuA',\n",
    "                     client_secret ='OZ0EsYse7J-JiYE7kPk3mddQgMZ2nA',\n",
    "                     user_agent ='WebScraper/Own-Biscotti6249')\n",
    "                     \n",
    "# to verify whether the instance is authorized instance or not\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data\n",
    "def date_to_timestamp(date_str):\n",
    "    return int(datetime.datetime.strptime(date_str, \"%Y-%m-%d\").timestamp())\n",
    "\n",
    "# gets posts within a subreddit and date range\n",
    "def fetch_submission_ids(subreddit_name, start_timestamp, end_timestamp):\n",
    "    submission_ids = []\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    print(f\"Fetching posts from r/{subreddit_name} between {start_timestamp} and {end_timestamp}...\")\n",
    "    \n",
    "    for submission in subreddit.new(limit=None):  # posts in chronological order \n",
    "        # dont get posts out of the desired time range\n",
    "        if submission.created_utc < start_timestamp:\n",
    "            break\n",
    "        if start_timestamp <= submission.created_utc <= end_timestamp:\n",
    "            print(f\"Found post: {submission.title} (ID: {submission.id})\")\n",
    "            submission_ids.append(submission.id)\n",
    "    \n",
    "    if not submission_ids:\n",
    "        print(\"No posts found in the specified date range.\")\n",
    "    return submission_ids\n",
    "\n",
    "# fetch comments and their replies\n",
    "def fetch_comments_and_replies(comment, submission_id, submission_title, start_timestamp, end_timestamp, depth=0):\n",
    "    if start_timestamp <= comment.created_utc <= end_timestamp:\n",
    "        comment_body = comment.body if comment.body != '[deleted]' else None\n",
    "        comment_author = comment.author.name if comment.author else None\n",
    "        \n",
    "        comment_data = {\n",
    "            \"submission_id\": submission_id,\n",
    "            \"submission_title\": submission_title,\n",
    "            \"comment_id\": comment.id,\n",
    "            \"comment\": comment_body,\n",
    "            \"author\": comment_author,\n",
    "            \"created\": datetime.datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"upvotes\": comment.score,\n",
    "            \"depth\": depth,\n",
    "            \"replies\": []\n",
    "        }\n",
    "\n",
    "        if comment.replies:\n",
    "            for reply in comment.replies:\n",
    "                if isinstance(reply, praw.models.Comment) and start_timestamp <= reply.created_utc <= end_timestamp:\n",
    "                    comment_data['replies'].append(fetch_comments_and_replies(reply, submission_id, submission_title, start_timestamp, end_timestamp, depth=depth+1))\n",
    "\n",
    "        return comment_data\n",
    "    return None\n",
    "\n",
    "# Fcomments from specific submissions\n",
    "def fetch_comments_from_submissions(submission_ids, start_timestamp, end_timestamp):\n",
    "    all_comments = []\n",
    "\n",
    "    for submission_id in submission_ids:\n",
    "        try:\n",
    "            submission = reddit.submission(id=submission_id)\n",
    "            print(f\"Fetching comments from: {submission.title} (ID: {submission.id})\")\n",
    "\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            for comment in submission.comments.list():\n",
    "                comment_data = fetch_comments_and_replies(comment, submission.id, submission.title, start_timestamp, end_timestamp)\n",
    "                if comment_data:\n",
    "                    all_comments.append(comment_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching comments from {submission_id}: {e}\")\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# save to JSON\n",
    "def save_comments_to_json(comments, json_file_name):\n",
    "    folder_path = 'reddit_data_new'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    json_file_path = os.path.join(folder_path, json_file_name)\n",
    "\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(comments, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Comments saved to {json_file_path}\")\n",
    "\n",
    "# putting it all together\n",
    "def main(subreddit_name, start_date, end_date, json_file_name):\n",
    "    start_timestamp = date_to_timestamp(start_date)\n",
    "    end_timestamp = date_to_timestamp(end_date)\n",
    "\n",
    "    # posts from the subreddit\n",
    "    submission_ids = fetch_submission_ids(subreddit_name, start_timestamp, end_timestamp)\n",
    "    \n",
    "    # comments from the sposts\n",
    "    comments = fetch_comments_from_submissions(submission_ids, start_timestamp, end_timestamp)\n",
    "    \n",
    "    # Save \n",
    "    save_comments_to_json(comments, json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script for democrats\n",
    "if __name__ == \"__main__\":\n",
    "    subreddit_name = \"democrats\"  \n",
    "    start_date = \"2024-11-06\"  \n",
    "    end_date = \"2024-11-18\"    \n",
    "    json_file_name = \"democrat_comments.json\"\n",
    "    \n",
    "    main(subreddit_name, start_date, end_date, json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script for republicans\n",
    "if __name__ == \"__main__\":\n",
    "    subreddit_name = \"republican\"  \n",
    "    start_date = \"2024-11-05\"  #\n",
    "    end_date = \"2024-11-19\"    \n",
    "    json_file_name = \"republican_comments.json\"\n",
    "    \n",
    "    main(subreddit_name, start_date, end_date, json_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c35cdab3726d60803cc3884986b4a5adc7ea73af240bf95fa669133b2d5e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
